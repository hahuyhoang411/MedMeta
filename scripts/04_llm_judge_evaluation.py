import argparse
import logging
import sys
import os
import time
import re
import getpass
import pandas as pd
from tqdm import tqdm
from typing import Optional, Tuple

# Ensure the src directory is in the Python path
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
sys.path.insert(0, PROJECT_ROOT)

# Project Modules (assuming config stores the output path of script 03)
from src.config import (
    EVALUATION_RESULTS_CSV_PATH, # Input for this script
    OUTPUT_DIR,
    LLM_CONFIG # Reuse LLM config if suitable, or define new ones
)

# LiteLLM for LLM calls
try:
    from litellm import completion
except ImportError:
    print("LiteLLM not installed. Please install it: pip install litellm")
    sys.exit(1)

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logging.getLogger("httpx").setLevel(logging.WARNING) # Reduce litellm verbosity

# --- LLM Judge Configuration ---
LLM_JUDGE_MODEL = "gemini/gemini-2.5-flash-preview-04-17" # Use model from config or default
LLM_JUDGE_MAX_TOKENS = 32000 # Max tokens for the judge's response
LLM_JUDGE_TEMPERATURE = 0.1 # Low temperature for consistent judging
LLM_JUDGE_REASONING_EFFORT = "low" # Or adjust as needed

# Output file for judged results
JUDGED_EVALUATION_RESULTS_CSV_PATH = os.path.join(OUTPUT_DIR, "medmeta_evaluation_llm_judged.csv")

# --- Helper Functions ---

def get_api_key(env_var="GEMINI_API_KEY"):
    """Gets the API key, prompting if not found in environment variables."""
    if env_var not in os.environ:
        logging.warning(f"{env_var} not found in environment variables.")
        try:
            os.environ[env_var] = getpass.getpass(f"Enter your Google AI API key ({env_var}): ")
        except Exception as e:
            logging.error(f"Could not get API key: {e}")
            return None
    api_key = os.environ.get(env_var)
    if not api_key:
        logging.error(f"API key ({env_var}) is missing or empty.")
        return None
    return api_key

def extract_score_and_reasoning(text: str) -> Tuple[Optional[int], Optional[str]]:
    """Extracts score and reasoning from the LLM judge response."""
    score = None
    reasoning = None

    # Extract Score
    score_match = re.search(r'Score:\s*(\d+)', text, re.IGNORECASE)
    if score_match:
        try:
            score = int(score_match.group(1))
        except ValueError:
            logging.warning(f"Could not parse score from match: {score_match.group(1)}")

    # Extract Reasoning (assuming it follows "Reasoning:")
    reasoning_match = re.search(r'Reasoning:\s*(.*)', text, re.IGNORECASE | re.DOTALL)
    if reasoning_match:
        reasoning = reasoning_match.group(1).strip()

    if score is None and reasoning is None:
         logging.warning(f"Could not extract score or reasoning from text: {text[:100]}...")
         # Fallback: Use the whole text as reasoning if extraction fails
         reasoning = text.strip() if text else "Extraction Failed"


    return score, reasoning

def get_llm_judgment(row: pd.Series, api_key: str) -> Tuple[Optional[int], Optional[str]]:
    """
    Sends data to the LLM judge and gets the score and reasoning.

    Args:
        row: A pandas Series representing a row from the evaluation results.
        api_key: The API key for the LLM provider.

    Returns:
        A tuple containing the extracted score (int) and reasoning (str).
    """
    original_conclusion = row.get('Conclusion', 'N/A')
    generated_conclusion = row.get('Generated Conclusion', 'N/A')

    # Define the system prompt for the LLM Judge
    system_prompt = """Y## Persona & Objective
You are an expert **Clinical Research Scientist and Critical Appraiser** specializing in meta-analysis methodology and scientific communication. Your objective is to rigorously evaluate the quality and semantic similarity of a conclusion generated by an Agentic RAG system (designed to automate meta-analysis) against the original conclusion from a published, peer-reviewed meta-analysis.

## Input Data
You will receive:
1.  `[Generated Conclusion]`: The text generated by the Agentic RAG system.
2.  `[Original Conclusion]`: The verbatim conclusion text from the published meta-analysis.

## Core Task
Evaluate the `[Generated Conclusion]` based on its semantic alignment and completeness compared to the `[Original Conclusion]`. Assign a similarity score from 0 to 5 using the detailed rubric below. Provide a structured justification for your score, referencing specific aspects of the comparison.

## Evaluation Criteria
Focus on the **semantic meaning and the core components** typically found in meta-analysis conclusions. Evaluate the `[Generated Conclusion]`'s alignment with the `[Original Conclusion]` across these dimensions:

1.  **Main Finding(s) / Overall Result:**
    *   Does it accurately capture the primary outcome(s) or effect(s) reported in the original? (e.g., treatment effectiveness, diagnostic accuracy, association strength, lack of effect).
    *   Does it reflect the direction and magnitude (if specified qualitatively or quantitatively) of the main finding(s)? (e.g., "significantly reduced LDL-C", "did not reduce mortality", "better clinical outcomes").
2.  **Key Specifics & Comparisons:**
    *   Does it mention the specific interventions, populations, or contexts discussed in the original? (e.g., "short-acting beta-blockers in septic patients", "pitavastatin at 1 mg, 2 mg, 4 mg", "ML based on high-resolution CT", "ACLR with ALLR").
    *   Does it include crucial comparisons highlighted in the original? (e.g., dose comparisons, comparison to other treatments/methods like "compared to isolated ACLR", "compared to less commonly used statins").
    *   Does it capture key quantitative results if present and central to the original conclusion? (e.g., "% reduction", specific sensitivity/specificity levels if mentioned).
3.  **Nuance, Caveats, and Limitations:**
    *   Does it reflect any major limitations, caveats, or calls for caution mentioned in the original? (e.g., "high heterogeneity", "interpret with caution", "further research needed", "remains uncertain", need for "multicenter clinical trials").
    *   Does it capture the level of certainty or confidence expressed in the original?
4.  **Implications & Future Directions:**
    *   Does it reflect the key clinical implications, significance, or recommendations for future research stated in the original? (e.g., "provide new insights into targeted treatment", "potential of AI-based tools", "support an individualized approach", "serve as a foundation for incorporation into clinical practice").
5.  **Safety/Tolerability (if applicable):**
    *   If the original conclusion addresses safety, adverse effects, or tolerability, does the generated conclusion accurately reflect this aspect? (e.g., "well tolerated and safe", "low incidence of myalgia", "increases in liver enzymes").
6.  **Overall Semantic Equivalence:**
    *   Considering all the above, does the generated conclusion convey the same core message and essential details as the original, even if the wording differs?

## Scoring Rubric (0-5 Scale)

*   **5: Excellent Similarity / Semantically Equivalent**
    *   Accurately captures all main findings, key specifics, essential nuance/caveats, and core implications/future directions from the original.
    *   Minor differences in wording are acceptable if the meaning is preserved entirely. Conveys the same overall message and takeaway points. Includes safety aspects if mentioned in the original.
    *   Essentially, a reader would draw the exact same conclusions from both texts regarding the study's outcome and significance.

*   **4: High Similarity / Mostly Equivalent**
    *   Accurately captures the main findings and most key specifics.
    *   May miss minor details, some nuance/caveats, or less critical implications OR phrase them slightly differently but without changing the core meaning.
    *   The primary takeaway message is the same.

*   **3: Moderate Similarity / Partially Equivalent**
    *   Captures the main finding(s) correctly but misses significant supporting details, comparisons, nuance, limitations, or implications mentioned in the original.
    *   OR captures most elements but introduces a minor inaccuracy or misrepresentation that slightly alters the emphasis or completeness.
    *   A reader gets the general gist but misses important context or qualifications present in the original.

*   **2: Low Similarity / Superficially Related**
    *   Captures *some* element related to the topic but misrepresents the main finding(s) or omits crucial information necessary to understand the original conclusion's core message.
    *   OR focuses on a minor point from the original while ignoring the central conclusion.
    *   There's a connection, but the essential meaning differs significantly.

*   **1: Very Low Similarity / Barely Related**
    *   Mentions the same general topic but the stated conclusions are substantially different, contradictory in parts, or completely miss the scope and findings of the original.
    *   Fails to capture almost all key evaluation criteria accurately.

*   **0: No Similarity / Contradictory or Irrelevant**
    *   The generated conclusion is on a completely different topic, directly contradicts the main findings of the original, or is nonsensical/irrelevant.

## Instructions for Evaluation

1.  **Read Carefully:** Thoroughly read and understand both the `[Generated Conclusion]` and the `[Original Conclusion]`.
2.  **Compare Systematically:** Evaluate the `[Generated Conclusion]` against the `[Original Conclusion]` using the **Evaluation Criteria** outlined above. Note points of alignment and divergence for each criterion.
3.  **Determine Score:** Based on your systematic comparison, select the score (0-5) from the **Scoring Rubric** that best reflects the overall semantic similarity and completeness.
4.  **Formulate Justification:** Write a concise yet comprehensive justification for your score.
    *   Start by stating the score.
    *   Explicitly reference the **Evaluation Criteria** (e.g., "The generated conclusion accurately captured the main finding regarding X but failed to mention the crucial caveat about heterogeneity...").
    *   Provide specific examples from both texts to support your assessment. Highlight key agreements and disagreements in meaning.
    *   Focus on *semantic content* rather than identical phrasing.

## Output Format

Provide your evaluation in the following structure:

Justification: [Your detailed explanation comparing the generated conclusion to the original conclusion based on the specified criteria and rubric. Explain *why* you assigned the score, referencing specific points of agreement and disagreement in their core message, details, nuance, and implications.]

Score: [Your score from 0-5]
"""

    # Define the user prompt content
    user_content = f"""
### Inputs
#### Generated Conclusion
{generated_conclusion}

#### Original Conclusion
{original_conclusion}
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content},
    ]

    try:
        response = completion(
            model=LLM_JUDGE_MODEL,
            messages=messages,
            api_key=api_key, # Pass the key directly
            max_tokens=LLM_JUDGE_MAX_TOKENS,
            temperature=LLM_JUDGE_TEMPERATURE,
            # reasoning_effort=LLM_JUDGE_REASONING_EFFORT # Optional parameter
        )
        response_text = response.choices[0].message.content
        logging.debug(f"LLM Judge Response: {response_text}")
        return extract_score_and_reasoning(response_text)

    except Exception as e:
        logging.error(f"Error calling LLM judge for conclusion '{original_conclusion[:50]}...': {e}", exc_info=True)
        return None, f"Error during LLM call: {type(e).__name__}"


# --- Main Function ---

def main(input_file: str, output_file: str, max_rows: Optional[int], wait_time: int):
    """
    Main function to load evaluation results, get LLM judgments, and save.

    Args:
        input_file: Path to the evaluation results CSV from script 03.
        output_file: Path to save the LLM-judged evaluation results CSV.
        max_rows: Maximum number of rows to process (None for all).
        wait_time: Seconds to wait between LLM calls (for rate limits).
    """
    judge_start_time = time.time()

    # --- Get API Key ---
    api_key = get_api_key() # Uses GEMINI_API_KEY by default
    if not api_key:
        logging.error("Cannot proceed without API key. Exiting.")
        return

    # --- Load Evaluation Data ---
    logging.info(f"Loading evaluation results from: {input_file}")
    if not os.path.exists(input_file):
        logging.error(f"Input evaluation file not found: {input_file}. Exiting.")
        return
    try:
        df_eval = pd.read_csv(input_file)
        logging.info(f"Loaded {len(df_eval)} rows for LLM judgment.")
    except Exception as e:
        logging.error(f"Failed to load evaluation CSV: {e}", exc_info=True)
        return

    # Limit rows if specified
    if max_rows is not None and max_rows > 0:
        logging.warning(f"Processing only the first {max_rows} rows for LLM judgment.")
        df_eval = df_eval.head(max_rows)
    elif max_rows == 0:
         logging.warning("Max rows set to 0. No LLM judgment will be performed.")
         return

    num_rows_to_process = len(df_eval)
    llm_scores = []
    llm_reasonings = []

    # --- Process Each Row ---
    logging.info(f"Starting LLM judgment loop for {num_rows_to_process} rows...")
    for index, row in tqdm(df_eval.iterrows(), total=num_rows_to_process, desc="Judging Rows"):
        start_row_time = time.time()
        original_number = row.get('Number', f'Index_{index}') # For logging

        logging.info(f"\n--- Judging Row {index+1}/{num_rows_to_process} (Number: {original_number}) ---")

        # Skip rows where the original conclusion was skipped or errored
        if str(row.get('Generated Conclusion', '')).startswith('Skipped') or \
           str(row.get('Generated Conclusion', '')).startswith('Error'):
            logging.warning(f"Skipping LLM judgment for row {index+1} due to previous skip/error.")
            score, reasoning = None, "Skipped - Prior Error/Skip"
        else:
            score, reasoning = get_llm_judgment(row, api_key)

        llm_scores.append(score)
        llm_reasonings.append(reasoning)

        end_row_time = time.time()
        logging.info(f"Row {index+1} judged in {end_row_time - start_row_time:.2f} seconds. Score: {score}")

        # Wait between rows if it's not the last row
        if wait_time > 0 and index < num_rows_to_process - 1:
            logging.info(f"Waiting for {wait_time} seconds before next row...")
            time.sleep(wait_time)

    # --- Add results and Save ---
    logging.info("LLM judgment loop finished.")
    df_eval['LLM Judge Score'] = llm_scores
    df_eval['LLM Judge Reasoning'] = llm_reasonings

    logging.info(f"Saving LLM-judged evaluation results to {output_file}...")
    try:
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        df_eval.to_csv(output_file, index=False, encoding='utf-8')
        logging.info("LLM-judged evaluation results saved successfully.")

        # Display summary
        print("\n--- LLM Judgment Summary ---")
        print(df_eval['LLM Judge Score'].value_counts(dropna=False).sort_index())
        avg_score = df_eval['LLM Judge Score'].mean()
        print(f"\nAverage LLM Judge Score: {avg_score:.2f}" if not pd.isna(avg_score) else "Average LLM Judge Score: N/A")

    except Exception as e:
        logging.error(f"Failed to save judged evaluation results: {e}", exc_info=True)

    judge_end_time = time.time()
    logging.info(f"Total LLM judgment script execution time: {judge_end_time - judge_start_time:.2f} seconds.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate RAG conclusions using an LLM judge.")
    parser.add_argument(
        "--input_file",
        default=EVALUATION_RESULTS_CSV_PATH,
        help=f"Path to the input evaluation results CSV (default: {EVALUATION_RESULTS_CSV_PATH})."
    )
    parser.add_argument(
        "--output_file",
        default=JUDGED_EVALUATION_RESULTS_CSV_PATH,
        help=f"Path to save the output CSV with LLM judgments (default: {JUDGED_EVALUATION_RESULTS_CSV_PATH})."
    )
    parser.add_argument(
        "--max_rows",
        type=int,
        default=None, # Process all rows by default
        help="Maximum number of rows to process from the input CSV (for debugging)."
    )
    parser.add_argument(
        "--wait_time",
        type=int,
        default=5, # Default wait time between API calls
        help="Seconds to wait between LLM API calls (adjust for rate limits)."
    )

    args = parser.parse_args()

    main(
        input_file=args.input_file,
        output_file=args.output_file,
        max_rows=args.max_rows,
        wait_time=args.wait_time
    )

    # Example usage from command line:
    # python scripts/04_llm_judge_evaluation.py
    # python scripts/04_llm_judge_evaluation.py --input_file ./output/medmeta_evaluation_results.csv --output_file ./output/judged_results.csv --max_rows 10 --wait_time 2